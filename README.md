# Predicting Students' Dropout and Academic Success

![istockphoto-1127339203-612x612](https://github.com/user-attachments/assets/3d1fa993-7850-4b90-b2dd-c280d168e44c)

![Language](https://img.shields.io/badge/language-Python-orange.svg)  
![Last Updated](https://img.shields.io/badge/last%20updated-May%202025-brightgreen)  
![Status](https://img.shields.io/badge/status-completed-green)


**Authors**: Andres Machado, Bret Geyer, Jackson Small, Jackson Colon, and Thomas Tibbetts<br>
*Spring 2025 | Data Science II*

#### Dataset Citation:
Realinho, Valentim, et al. "Predict Students' Dropout and Academic Success." UCI Machine Learning Repository, 2021, https://doi.org/10.24432/C5MC89.

#### Dataset Source:
https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success

## üìö Table of Contents
- [Project Statement](#project-statement)
- [Description of Data](#description-of-data)
- [Data Exploration](#data-exploration)
- [Data Visualization](#data-visualization)
- [Feature Selection](#feature-selection)
- [Preliminary Modeling](#preliminary-modeling)
- [Tuning Individual Models](#tuning-individual-models)
- [Consideration of Oversampling Methods (SMOTE)](#consideration-of-oversampling-methods-smote)
- [Ensemble Methods](#ensemble-methods)
- [Project Trajectory](#project-trajectory)
- [Conclusion](#conclusion)
- [Resources](#resources)
  
## üõ†Ô∏è How to Run
Navigate to the [code](https://github.com/Andres04Machado/Student_Outcome_Classsification/tree/main/code) folder and follow these steps
1. **Download** the `.ipynb` notebook file `Student_Outcome_Classification.ipynb`.
2. **Open** the notebook using [Jupyter Notebook](https://jupyter.org/install) or [JupyterLab](https://jupyterlab.readthedocs.io/en/stable/).
3. Run
   
```bash
pip install -r requirements.txt
```

4. **Run all cells to your heart's content!**

*Note: If you would like to see any of the plots or charts as `.png` files, please check out the [plots_charts](https://github.com/Andres04Machado/Student_Outcome_Classsification/tree/main/plots_charts) folder.*
## Project Statement
‚Ä≠Student retention is a major problem for institutes of higher education, with many‚Ä¨ resources devoted to analyzing what causes students to drop out or graduated late. When the‚Ä¨ graduation rate falls, so does a school‚Äôs ranking and reputation. For this reason it‚Äôs desirable to‚Ä¨ reach out to these students early in their academic path, so they can receive extra counseling or‚Ä¨ attention that may improve their situation. For this project, we attempt to use data collected from‚Ä¨ ‚Ä≠university students to predict whether they will be graduates, dropouts, or still enrolled at the‚Ä¨ end of a typical program term. Some variables of particular interest are the student‚Äôs age at‚Ä¨ ‚Ä≠enrollment, scholarship status, and early grades. We will use classification algorithms with‚Ä¨ considerations for reducing class imbalance. Our goal was to build a classification model that identifies students at risk of not graduating, enabling proactive intervention.
‚Ä≠
## Description of Data:‚Ä¨
‚Ä≠Broadly speaking, this project's focus is predicting the academic success of university‚Ä¨ students. We are working with a tabular dataset which lists 37 attributes for each of 4,424‚Ä¨ students at the Polytechnic Institute of Portalegre, and contains 18 categorical features‚Ä¨ which encode information such as the student‚Äôs program, marital status, application mode, and‚Ä¨ their parents‚Äô level of education. Some of the quantitative variables encode the student‚Äôs age‚Ä¨ and numerical evaluations (0-100) of their admission profile and previous qualifications, and‚Ä¨ other quantitative variables which detail the number of curricular units for which the student was‚Ä¨ enrolled, approved, and credited during their first two semesters. The dataset‚Äôs target variable is‚Ä¨ a categorical column which designates each student as one of the following:‚Ä¨

* **Enrolled**‚Ä¨‚Ä≠: Indicating that the student was still enrolled‚Ä¨‚Ä≠ (not graduated) at the end of the‚Ä¨ normal term of the program. Where 794 students classify as Enrolled.‚Ä¨
* **Graduated‚Ä¨‚Ä≠**: Indicating the student graduated by the‚Ä¨‚Ä≠ end of the normal term of the‚Ä¨ program. Where 2,209 students classify as Graduated.‚Ä¨
* **Dropout**‚Ä¨‚Ä≠: Indicating that the student dropped out by‚Ä¨‚Ä≠ the end of the program‚Äôs normal‚Ä¨ term. Where 1421 students classify as Dropouts.‚Ä¨

More information about feature definitions and dataset origins can be found here:‚Ä¨
https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success‚Ä¨

## Data Exploration:
One of the dataset‚Äôs issues is containing many categorical features, each of which‚Ä¨ consists of many different levels. For example, the ‚ÄúApplication mode‚Äù variable alone has 18‚Ä¨ levels, some of which apply to fewer than 10 students. If all of these categorical features were‚Ä¨ one-hot encoded, they would add hundreds of sparse columns to the dataset and create‚Ä¨ unnecessary bulk during the data exploration. For this reason, it was decided that all categorical‚Ä¨ feature levels representing less than 2% of the dataset would be binned into one level called‚Ä¨ ‚ÄúOther‚Äù. This significantly reduced the unnecessary complexity in the dataset.‚Ä¨

‚Ä≠While checking the data, we also noticed that there were 180 students that were listed as having‚Ä¨ enrolled in 0 curricular units during both their first and second semester. This may be attributed‚Ä¨ to faulty data collection or other unusual circumstances. For the purpose of predicting student‚Ä¨ success, we considered these records to be highly anomalous and decided to remove them‚Ä¨ from the dataset. After this removal, the dataset contained 4,244 records.
‚Ä≠
‚Ä≠It‚Äôs important to note that the target classes are‚Ä¨ imbalanced. About 50% of the students are graduates,‚Ä¨ 18% are enrolled, and 31.7% are dropouts. This will‚Ä¨ inform our modeling and experimental design. To‚Ä¨ account for the unbalanced classes, it may be‚Ä¨ necessary to use methods like Stratified K-Folds or‚Ä¨ SMOTE.‚Ä¨
‚Ä≠

‚Ä¨<img width="225" alt="Screenshot 2025-05-28 at 10 33 04‚ÄØAM" src="https://github.com/user-attachments/assets/d4783848-c898-44a3-8f12-1d68b9c176bf" />

## Data Visualization:
Using `seaborn`, `plotly`, and `matplotlib` we looked to understand the features that influence the students final academic outcome. We first visualized key‚Ä¨ features in relation to the target variable. Each of the following plots helped us gain a deeper‚Ä¨ knowledge of the demographics, financial, and academic patterns that distinguish students from‚Ä¨ graduates, dropouts, and currently enrolled.‚Ä¨
‚Ä≠ 

We discovered that dropout students had a heavily right-skewed‚Ä¨ distribution of age of enrollment as compared to the other classes. We also noticed that within‚Ä¨ the first semester grades variable, a clear cluster of dropouts had grades centered around 0. In‚Ä¨ contrast, those students who graduated displayed a more balanced and higher spread of‚Ä¨ grades


<img width="559" alt="Screenshot 2025-05-28 at 11 37 26‚ÄØAM" src="https://github.com/user-attachments/assets/eebbc88e-4a0d-4718-992f-14a9ae24283f" />


The ‚ÄúTuition fees up to date‚Äù variable is a binary feature tracking whether the student‚Ä¨ had fully paid their tuition fees at the time of data collection. In the chart below, it can be seen‚Ä¨ that a high proportion of students who didn‚Äôt keep up with their tuition fees became dropouts.‚Ä¨ This finding underscores the significant role financial hardship plays in student retention.. Additionally, it was found that students who held scholarships were much more likely‚Ä¨ to graduate than those who did not. Overall, these visualizations capture risk factors associated‚Ä¨ with students who dropped out, graduated or are currently enrolled, but also highlight how early‚Ä¨ academic performance and financial support are needed for students' success.‚Ä¨
‚Ä≠

<img width="660" alt="Screenshot 2025-05-28 at 11 41 57‚ÄØAM" src="https://github.com/user-attachments/assets/1cb24e63-d1a4-401a-adc9-da39955bd7cf" />

## Feature Selection:
When starting the feature selection, it was essential to consider how to handle each‚Ä¨ main variable data type-categorical/nominal and numerical. For each of these data types,‚Ä¨ feature selection was treated differently; for categorical features, we use Cramer's V statistic,‚Ä¨ which utilises the Chi-squared statistic and transformations to provide a value between 0 and 1‚Ä¨ for a pair of categorical features. Alternatively, for numerical variables, we used Pearson‚Äôs‚Ä¨ Correlation Coefficient and the SelectKBest algorithm with the ANOVA F-stat scoring function.‚Ä¨
‚Ä≠ 

One observation worth noting was a high degree of multicollinearity between‚Ä¨ 1st-semester features and their 2nd-semester counterparts. To generalize our features as much‚Ä¨ as possible, we opted to keep the 1st-semester features. We also dropped features that were‚Ä¨ not clearly defined when we collected the data.‚Ä¨
‚Ä≠

Of the 17 categorical features the dataset provided, 14 were selected. However, out of‚Ä¨ the 19 numerical features the dataset provided, 7 were selected; meaning, we began with 36‚Ä¨ features, but settled on 21 once selection was complete.‚Ä¨
‚Ä≠

## Preliminary Modeling:
We established a baseline of evaluation metrics for several different classification‚Ä¨ techniques, each using 5-fold stratified cross-validation on the dataset. The models were‚Ä¨ initialized without tuning of hyperparameters. The results are summarized in the table below:


<img width="687" alt="Screenshot 2025-05-28 at 12 05 21‚ÄØPM" src="https://github.com/user-attachments/assets/5e66cc8b-3b88-4ee9-99a0-15531d8ed2f7" />


While evaluating the baseline models, we noticed that the ‚ÄúEnrolled‚Äù class had the‚Ä¨ lowest F1-score across all algorithms. This could be due to it being the smallest minority class in‚Ä¨ the data. Further modeling may benefit from using oversampling methods like SMOTE to‚Ä¨ balance the classes in the training set.‚Ä¨
‚Ä≠ 

We also noted that among the baseline models, the Gaussian Naive Bayes model‚Ä¨ obtained the best recall for the ‚ÄúEnrolled‚Äù class by a significant margin. However, we also saw‚Ä¨ that the Naive Bayes model underperformed in essentially every other metric. The best overall‚Ä¨ baseline performance was obtained by the Logistic Regression and SVM models. Both showed‚Ä¨ a relatively high accuracy of 0.76, and captured the ‚ÄúGraduate‚Äù class similarly well. They differ in‚Ä¨ that the Logistic Regression model achieved a higher recall of the ‚ÄúDropout‚Äù class, while the‚Ä¨ SVM achieved higher recall of the ‚ÄúEnrolled‚Äù class. We would like to tune the hyperparameters‚Ä¨ of each of these models, check if oversampling the training data improves performance, then try‚Ä¨ to combine their individual strengths through an ensemble model.


## Tuning Individual Models:

We tried to improve each individual model as much as possible before combining them‚Ä¨ into a final ensemble model. This consisted of tuning each model‚Äôs hyperparameters. We used‚Ä¨‚Ä≠ the weighted F1 score as our metric because of our class imbalances. Below is a summary of‚Ä¨‚Ä≠ the results of hyperparameter tuning each model:‚Ä¨

- <ins>Logistic Regression</ins> : The most substantial opportunity‚Ä¨‚Ä≠ for hypertuning this model is‚Ä¨ manually adjusting the impact of a regularization term, in order to prevent overfitting to‚Ä¨‚Ä≠ the training data. We found that after hyperparameter tuning with GridSearchCV, the L1 penalty is‚Ä¨‚Ä≠ optimal for our Logistic Regression model which promotes sparsity in our coefficients.‚Ä¨‚Ä≠ The optimal parameters included a C value of 10, maximum iterations of 500, and using‚Ä¨‚Ä≠ the ‚Äòsaga‚Äô solver instead of ‚Äòliblinear‚Äô. The weighted F1 score for our optimal model is‚Ä¨ 0.73. However, although overall model accuracy increased, the predictions for the‚Ä≠ Enrolled class still lagged behind.‚Ä¨
- <ins>Support Vector Machine</ins>: The Support Vector Machine‚Ä¨‚Ä≠ allows the use of different kernel‚Ä¨ functions, rather than the default radial kernel. It also permits adjustment of the‚Ä¨ parameter C, which controls the penalty of margin violations and thus affects the width of‚Ä¨ the margin. We found that after hypertuning with GridSearchCV, we found that a C value‚Ä¨ of 10 was optimal. Also, the radial kernel remained as the best kernel and leaving the‚Ä¨ ‚Äògamma‚Äô parameter as ‚Äòauto‚Äô made the model optimal. The weighted F1 score for our‚Ä¨‚Ä≠ optimal model was also 0.74.
- <ins>Naive Bayes</ins>: Due to the nature of the algorithm, there was not much flexibility to tune‚Ä¨ the Gaussian Naive Bayes model. For this reason, we decided to simply pass it through‚Ä¨‚Ä≠ with default parameters to the later modeling processes.


## Consideration of Oversampling Methods (SMOTE):
Suspecting that the imbalance of classes was behind the lackluster prediction of the‚Ä¨ ‚ÄúEnrolled‚Äù class, we considered the use of oversampling methods to rebalance classes in the‚Ä¨‚Ä≠ training data. We specifically used the SMOTE (Synthetic Minority Over-Sampling Technique)‚Ä¨‚Ä≠ algorithm to implement this rebalancing. Using the optimal hyperparameters we found in the‚Ä¨‚Ä≠ previous section, we experimented with using SMOTE during the training of each model to see‚Ä¨‚Ä≠ if it improved performance, particularly on the ‚ÄúEnrolled‚Äù minority class.‚Ä¨
‚Ä≠
We found that over the three models when we applied SMOTE, the Enrolled class‚Äô F1‚Ä¨‚Ä≠ score increased. However, scores for the other classes decreased, with Naive Bayes taking the‚Ä¨‚Ä≠ most dramatic hit. The score changes went as followed for each model:

<img width="507" alt="Screenshot 2025-05-28 at 12 52 01‚ÄØPM" src="https://github.com/user-attachments/assets/5d8584f7-abb0-4a5c-9735-d30f81ef88fb" />


As we can see, the effect of hypertuning and SMOTE had a positive effect on increasing‚Ä¨‚Ä≠ the accuracy of the Enrolled class. This however came at a cost of lowering scores for other‚Ä¨‚Ä≠ classes. For the sake of having a more balanced prediction, it may be more beneficial to choose‚Ä¨‚Ä≠ the Logistic Regression with SMOTE as our best model. The F1 score for the Enrolled class‚Ä¨‚Ä≠ increased the most while the other two classes didn‚Äôt change dramatically. This experiment was‚Ä¨‚Ä≠ worthwhile as we see that using a different resampling method like SMOTE helped increase‚Ä¨‚Ä≠ model accuracy. Let‚Äôs try one more thing to see if we increase classification predictability


## Ensemble Methods:
As previously mentioned, each of the Logistic Regression, Naive Bayes, and SVM‚Ä¨‚Ä≠ models displayed their own strengths and weaknesses. We thought that the best way to‚Ä¨‚Ä≠ aggregate the advantages of each model would be to implement an ensemble model that‚Ä¨‚Ä≠ averaged the predictions of each algorithm. For this objective, we used sci-kit learn‚Äôs Voting‚Ä¨‚Ä≠ Classifier model with the ‚Äúsoft voting‚Äù option. Soft voting enables the ensemble to make a‚Ä¨‚Ä≠ decision by averaging each class‚Äôs predicted probability among all of the constituent models.‚Ä¨
‚Ä≠ 
The voting classifier accepts a weight parameter, useful for emphasizing the predictions‚Ä¨‚Ä≠ of one constituent model over another. Using a grid search with 5-fold stratified cross-validation,‚Ä¨‚Ä≠ we optimized the weight vector which resulted in an emphasis on the Logistic Regression and‚Ä¨‚Ä≠ SVM components with equal weights and a smaller weight on the Naive Bayes model. After‚Ä¨‚Ä≠ defining a tuned voting classifier model and training on SMOTE-augmented data, it achieved the‚Ä¨‚Ä≠ following results on the test set (equivalent results are displayed for the individual models for‚Ä¨‚Ä≠ comparison purposes:


<img width="681" alt="Screenshot 2025-05-28 at 12 56 35‚ÄØPM" src="https://github.com/user-attachments/assets/24793338-9424-464b-9f16-4275be19a260" />

It can be seen that the results of the Voting Classifier differed only minutely from the‚Ä¨ individually-tuned Logistic Regression model.‚Ä¨
‚Ä≠

## Project Trajectory:
At first, we focused on testing baseline models and evaluating them using classification‚Ä¨‚Ä≠ reports, paying attention to precision, recall, F1 score, and accuracy. However, a key challenge‚Ä¨ with this dataset was the class imbalance, clearly shown in the pie chart (found in Data Exploration section). Traditional‚Ä¨‚Ä≠ classification metrics can be misleading under imbalance, as they often favor the majority class.‚Ä¨

‚Ä≠This issue became evident in our baseline results: the Graduate and Dropout classes‚Ä¨‚Ä≠ performed well, while the Enrolled class was consistently misclassified. Recognizing this, we shifted our focus toward identifying models that improve performance across all classes,‚Ä¨‚Ä≠ particularly the Enrolled class.‚Ä¨

To better capture balanced performance, we adopted the weighted F1 score as our‚Ä¨‚Ä≠ primary metric, as it accounts for both precision and recall while weighting by support. From‚Ä¨‚Ä≠ here, we implemented hyperparameter tuning, SMOTE, and a voting classifier to address class‚Ä¨‚Ä≠ imbalance more effectively. We then tracked F1 and weighted F1 scores to measure‚Ä¨‚Ä≠ improvements over the baseline. Ultimately, we prioritized balanced performance across all‚Ä¨‚Ä≠ classes over chasing a single high metric, leading to more meaningful model improvements.‚Ä¨
‚Ä≠
## Conclusion:
‚Ä≠In conclusion, we wanted to predict the student‚Ä¨‚Ä≠ academic outcomes of either‚Ä≠ graduation, dropout, or enrollment by using financial, demographical, and academic‚Ä¨‚Ä≠ performance data. After testing our models, the Logistic Regression model achieved the‚Ä¨‚Ä≠ highest weighted F1-score of 0.74. We applied SMOTE to improve our model‚Ä¨‚Ä≠ performance by mitigating class imbalance to enhance the low recall for the small‚Ä¨ ‚ÄúEnrolled‚Äù class. We also tried to bolster the predictive power by ensembling Logistic‚Ä¨‚Ä≠ Regression, the Support Vector Classifier, and the Gaussian Naive Bayes models into a‚Ä¨‚Ä≠ soft voting classifier.

- **Strengths**: SMOTE and hyperparameter tuning led to a more balanced training set and‚Ä¨‚Ä≠ a more reliable classification of the minority classes. Our models were able to predict‚Ä¨ ‚ÄúGraduated‚Äù and ‚ÄúDropped‚Äù students with high precision and recall.‚Ä¨
- **Short-comings**: Even our most highly-tuned models didn‚Äôt‚Ä¨‚Ä≠ surpass a total accuracy of‚Ä¨‚Ä≠ 75% on the test set. Even with considerations for target class imbalance like SMOTE,‚Ä¨‚Ä≠ our classification models achieved less than 70% recall on the Dropout class and less‚Ä¨‚Ä≠ than 60% recall for the Enrolled class. This means that a large number of at-risk‚Ä¨‚Ä≠ students were not identified in the test set, and could potentially slip through the cracks.
- **Future work**: We could potentially address our project‚Äôs‚Ä¨‚Ä≠ shortcomings by reframing the‚Ä¨‚Ä≠ classification task. Rather than predicting the Enrolled and Dropout classes separately,‚Ä¨‚Ä≠ we could combine them into a single class and treat it as a binary classification problem‚Ä¨‚Ä≠ between ‚ÄúGraduate‚Äù and ‚ÄúNon-graduate‚Äù. If we accept the assumption that ‚ÄúEnrolled‚Äù‚Ä≠ students and ‚ÄúDropout‚Äù students can benefit from the same extra assistance, then it‚Ä¨‚Ä≠ might serve the objective better to combine these classes. In addition, if this framework‚Ä¨‚Ä≠ were pursued, then the ‚ÄúGraduate‚Äù and ‚ÄúNon-graduate‚Äù target classes would have‚Ä¨‚Ä≠ almost a completely even 50/50 split, which might encourage better predictive accuracy.

#### Resources:
‚Ä¨‚Ä≠Cramer‚Äôs V Origination: Cram√©r, Harald. 1946. Mathematical Methods of Statistics. Princeton: Princeton‚Ä¨
‚Ä≠University Press, page 282 (Chapter 21. The two-dimensional case).‚Ä¨

Wikipedia Article with Cramer‚Äôs V Formula:‚Ä¨‚Ä≠ 
https://en.wikipedia.org/wiki/Cram%C3%A9r's_V

‚Ä≠Medium Article going into using Cramer‚Äôs V:‚Ä¨
‚Ä≠https://medium.com/@manindersingh120996/understanding-categorical-correlations-with-chi-square-test-and-cramers-v-a54fe153b1d6‚Ä¨

‚Ä≠SelectKBest() documentation:‚Ä¨ 
https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html‚Ä¨
